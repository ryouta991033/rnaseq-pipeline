################################
# workflow/Snakefile (Docker安定版 + S3ログ同期 修正版)
################################

import os
import pandas as pd
import glob

# -----------------------------
# プロジェクトルート
# -----------------------------
PROJECT_DIR = os.getcwd()

configfile: "config/config.yaml"

# -----------------------------
# サンプル読み込み
# -----------------------------
SAMPLES_TSV = "samples.tsv"
samples = pd.read_csv(SAMPLES_TSV, sep="\t")

SAMPLES = samples["sample"].tolist()

SE_SAMPLES = samples.query("layout == 'SE'")["sample"].tolist()
PE_SAMPLES = samples.query("layout == 'PE'")["sample"].tolist()

# -----------------------------
# FastQ 自動探索関数
# （※将来使う場合に備えて安全化）
# -----------------------------
def fq1(wildcards):
    sample = wildcards.sample
    files = glob.glob(f"data/fastq/{sample}/*.fastq.gz")
    if not files:
        raise ValueError(f"No FastQ file found for sample {sample}")
    for f in files:
        if "R1" in f or "r1" in f or len(files) == 1:
            return str(f)
    return str(files[0])

def fq2(wildcards):
    sample = wildcards.sample
    files = glob.glob(f"data/fastq/{sample}/*.fastq.gz")
    r2_files = [f for f in files if "R2" in f or "r2" in f]
    return [str(f) for f in r2_files] if r2_files else []

# -----------------------------
# ルール読み込み
# -----------------------------
include: "rules/trim.smk"
include: "rules/build_index.smk"
include: "rules/align.smk"
include: "rules/count.smk"

# -----------------------------
# mapper に応じた index 指定
# -----------------------------
MAPPER = config.get("mapper", "star")

if MAPPER == "star":
    INDEX_TARGET = "results/reference/STAR_index/Genome"
elif MAPPER == "hisat2":
    INDEX_TARGET = "results/reference/hisat2_index/genome.1.ht2"
else:
    raise ValueError(f"Unsupported mapper: {MAPPER}")

# -----------------------------
# S3 設定
# -----------------------------
AWS_BUCKET = config["aws"]["bucket"]
AWS_PREFIX = config["aws"]["results_prefix"]

RUN_LOG = "results/snakemake_run.log"
STATS_FILE = "results/snakemake_stats.json"

# -----------------------------
# S3 同期ルール（ダミー出力付き）
# -----------------------------
rule sync_s3:
    input:
        "results/deseq2/DESeq2_results_all.csv",
        "results/deseq2/DESeq2_results_significant.csv",
        "results/deseq2/DESeq2_PCA.png"
    output:
        "results/.s3_sync_done"
    shell:
        r"""
        set -euo pipefail

        echo "[INFO] Syncing results directory..."
        aws s3 sync results \
            s3://{AWS_BUCKET}/{AWS_PREFIX}/results \
            --delete

        echo "[INFO] Syncing snakemake internal logs..."
        if [ -d ".snakemake/log" ]; then
            aws s3 sync .snakemake/log \
                s3://{AWS_BUCKET}/{AWS_PREFIX}/snakemake_logs \
                --delete
        fi

        if [ -f "{RUN_LOG}" ]; then
            aws s3 cp {RUN_LOG} \
                s3://{AWS_BUCKET}/{AWS_PREFIX}/snakemake_run.log
        fi

        if [ -f "{STATS_FILE}" ]; then
            aws s3 cp {STATS_FILE} \
                s3://{AWS_BUCKET}/{AWS_PREFIX}/snakemake_stats.json
        fi

        touch {output}

        echo "[INFO] S3 synchronization complete."
        """

# -----------------------------
# rule all（正しい依存関係）
# -----------------------------
rule all:
    input:
        # インデックス
        INDEX_TARGET,

        # BAM + index
        expand("results/bam/{sample}.bam", sample=SAMPLES),
        expand("results/bam/{sample}.bam.bai", sample=SAMPLES),

        # featureCounts
        "results/counts/gene_counts.txt",

        # DESeq2 出力
        "results/deseq2/DESeq2_results_all.csv",
        "results/deseq2/DESeq2_results_significant.csv",
        "results/deseq2/DESeq2_PCA.png",

        # S3 同期完了フラグ
        "results/.s3_sync_done"

